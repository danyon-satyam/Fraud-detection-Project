{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìò Jupyter Notebook: Text Extraction and Analysis\n",
    "=====================================\n",
    "Author: Satyam Mohapatra\n",
    "-------------------------------------\n",
    "Date: 2024-12-21\n",
    "-------------------------------------\n",
    "Objective:\n",
    "To extract article text from URLs provided in Input.xlsx, clean the text using customized stopwords, and perform sentiment and readability analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Table of Contents:\n",
    "   1. Introduction\n",
    "   2. Setup and Imports\n",
    "   3. Data Extraction\n",
    "   4. Text Cleaning and Stopwords\n",
    "   5. Sentiment and Readability Analysis\n",
    "   6. Saving Results\n",
    "   7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. üìñ Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "    This notebook automates the process of web article extraction and analysis. Key tasks include:\n",
    "    - Extracting text from URLs.\n",
    "    - Cleaning and preprocessing the text by removing stopwords (currencies, dates, generic words, etc.).\n",
    "    - Performing sentiment analysis (positive/negative scores, polarity, subjectivity).\n",
    "    - Computing readability metrics (fog index, sentence length, word count).\n",
    "    - Exporting results to Output Data Structure.xlsx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ‚öôÔ∏è Setup and Imports <a id=\"setup-and-imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (4.27.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2024.12.14)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: syllapy in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (0.7.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: setuptools in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (75.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages:\n",
    "%pip install beautifulsoup4\n",
    "%pip install selenium \n",
    "%pip install nltk \n",
    "%pip install openpyxl \n",
    "%pip install pandas \n",
    "%pip install syllapy \n",
    "%pip install requests\n",
    "%pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smt93\\Test Assignment\\Data Extraction and NLP Blackcoffer\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries:\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "print(os.getcwd())\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import syllapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smt93\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smt93\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. üåê Data Extraction <a id=\"data-extraction\"></a>\n",
    "\n",
    "    Goal: Extract text content from URLs provided in Input.xlsx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "# Load URLs from Excel\n",
    "df = pd.read_excel(r\"C:\\Users\\smt93\\Test Assignment\\Input.xlsx\")\n",
    "df.to_csv(r\"C:\\Users\\smt93\\Test Assignment\\Input.csv\", index=False)\n",
    "\n",
    "# Directory to store extracted articles\n",
    "os.makedirs('extracted_articles', exist_ok=True)\n",
    "\n",
    "# Extract article text from URL\n",
    "def extract_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "        return article_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract {url}: {e}\")\n",
    "        return ''\n",
    "    \n",
    "# Extract and save articles\n",
    "for index, row in df.iterrows():\n",
    "    article = extract_article(row['URL'])\n",
    "    with open(f\"extracted_articles/{row['URL_ID']}.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ‚úÇÔ∏è Text Cleaning and Stopwords <a id=\"text-cleaning-and-stopwords\"></a>\n",
    "\n",
    "    Goal: Remove irrelevant words (stopwords) from extracted text using custom stopword lists.\n",
    "    \n",
    "    Stopwords Include:\n",
    "    - Auditor names\n",
    "    - Currencies\n",
    "    - Dates and numbers\n",
    "    - Generic terms\n",
    "    - Geographic locations\n",
    "    - Common names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code: \n",
    "stopwords_auditor = set(open(r\"C:\\Users\\smt93\\Test Assignment\\StopWords\\StopWords_Auditor.txt\").read().split())\n",
    "\n",
    "with open(r\"C:\\Users\\smt93\\Test Assignment\\StopWords\\StopWords_Currencies.txt\", 'rb') as file:\n",
    "    byte_content = file.read()\n",
    "\n",
    "# Decode with error handling\n",
    "decoded_content = byte_content.decode('utf-8', errors='replace')\n",
    "stopwords_currencies = set(decoded_content.split())\n",
    "\n",
    "stopwords_dates_numbers = set(open(r\"C:\\Users\\smt93\\Test Assignment\\StopWords\\StopWords_DatesandNumbers.txt\").read().split())\n",
    "stopwords_generic = set(open(r\"C:\\Users\\smt93\\Test Assignment\\StopWords\\StopWords_Generic.txt\").read().split())\n",
    "stopwords_genericlong = set(open(r\"C:\\Users\\smt93\\Test Assignment\\StopWords\\StopWords_GenericLong.txt\").read().split())\n",
    "stopwords_geographic = set(open(r\"C:\\Users\\smt93\\Test Assignment\\StopWords\\StopWords_Geographic.txt\").read().split())\n",
    "stopwords_names = set(open(r\"C:\\Users\\smt93\\Test Assignment\\StopWords\\StopWords_Names.txt\").read().split())\n",
    "\n",
    "custom_stopwords = (\n",
    "    stopwords_auditor.union(\n",
    "        stopwords_currencies,\n",
    "        stopwords_dates_numbers,\n",
    "        stopwords_generic,\n",
    "        stopwords_genericlong,\n",
    "        stopwords_geographic,\n",
    "        stopwords_names\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. üìä Sentiment and Readability Analysis <a id=\"sentiment-and-readability-analysis\"></a>\n",
    "\n",
    "   Goal: Perform sentiment and readability analysis to compute the following variables:\n",
    "\n",
    "   - POSITIVE SCORE ‚Äì Total count of positive words.\n",
    "   - NEGATIVE SCORE ‚Äì Total count of negative words.\n",
    "   - POLARITY SCORE ‚Äì Measures overall positivity or negativity of the text.\n",
    "   - SUBJECTIVITY SCORE ‚Äì Indicates how subjective or objective the text is.\n",
    "   - AVG SENTENCE LENGTH ‚Äì Average number of words per sentence.\n",
    "   - PERCENTAGE OF COMPLEX WORDS ‚Äì Proportion of words with more than two syllables.\n",
    "   - FOG INDEX ‚Äì Readability score indicating text complexity.\n",
    "   - AVG NUMBER OF WORDS PER SENTENCE ‚Äì Average word count across sentences.\n",
    "   - COMPLEX WORD COUNT ‚Äì Total count of words with more than two syllables.\n",
    "   - WORD COUNT ‚Äì Total number of words (excluding stopwords).\n",
    "   - SYLLABLE PER WORD ‚Äì Average syllable count per word.\n",
    "   - PERSONAL PRONOUNS ‚Äì Count of personal pronouns like I, we, my, ours, us.\n",
    "   - AVG WORD LENGTH ‚Äì Average character length of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "positive_words = set(open(r\"C:\\Users\\smt93\\Test Assignment\\MasterDictionary\\positive-words.txt\").read().split())\n",
    "\n",
    "with open(r\"C:\\Users\\smt93\\Test Assignment\\MasterDictionary\\negative-words.txt\", 'rb') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Decode with error handling\n",
    "decoded_content1 = content.decode('utf-8', errors='replace')\n",
    "negative_words = set(decoded_content1.split())\n",
    "\n",
    "pos_dict = set(positive_words) \n",
    "neg_dict = set(negative_words)\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha() and word not in custom_stopwords]\n",
    "    \n",
    "    # 1. Sentiment Analysis\n",
    "    positive_score = sum(1 for word in words if word in pos_dict)\n",
    "    negative_score = sum(1 for word in words if word in neg_dict)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "    \n",
    "    # 2. Readability and Complexity Analysis\n",
    "    if len(sentences) > 0:\n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "    else:\n",
    "        avg_sentence_length = 0\n",
    "    \n",
    "    complex_words = [word for word in words if syllapy.count(word) > 2]\n",
    "    percentage_complex = len(complex_words) / len(words) if len(words) > 0 else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex)\n",
    "    \n",
    "    # 3. Additional Metrics\n",
    "    avg_number_of_words_per_sentence = len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "    complex_word_count = len(complex_words)\n",
    "    word_count = len(words)\n",
    "    syllable_per_word = sum(syllapy.count(word) for word in words) / word_count if word_count > 0 else 0\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        'POSITIVE SCORE': positive_score,\n",
    "        'NEGATIVE SCORE': negative_score,\n",
    "        'POLARITY SCORE': polarity_score,\n",
    "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex,\n",
    "        'FOG INDEX': fog_index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': avg_number_of_words_per_sentence,\n",
    "        'COMPLEX WORD COUNT': complex_word_count,\n",
    "        'WORD COUNT': word_count,\n",
    "        'SYLLABLE PER WORD': syllable_per_word,\n",
    "        'PERSONAL PRONOUNS': personal_pronouns,\n",
    "        'AVG WORD LENGTH': avg_word_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. üíæ Saving Results <a id=\"saving-results\"></a>\n",
    "\n",
    "    Goal: Save analysis results to an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'Output Data Structure.xlsx'\n"
     ]
    }
   ],
   "source": [
    "#Code:\n",
    "input_df = pd.read_excel(r\"C:\\Users\\smt93\\Test Assignment\\Input.xlsx\")  # Load input data\n",
    "results = []\n",
    "\n",
    "for _, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    file_path = f'extracted_articles/{url_id}.txt'\n",
    "    \n",
    "    # Process only if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                \n",
    "                if len(text.strip()) == 0:  # Skip empty files\n",
    "                    print(f\"Skipping empty file: {url_id}.txt\")\n",
    "                    continue\n",
    "\n",
    "                # Perform Sentiment and Readability Analysis\n",
    "                analysis = analyze_sentiment(text)\n",
    "                result = {\n",
    "                    'URL_ID': row['URL_ID'],      # From Input.xlsx\n",
    "                    'URL': row['URL'],            # From Input.xlsx\n",
    "                    **analysis                     # Add computed metrics\n",
    "                }\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Convert Results to DataFrame and Save\n",
    "output_df = pd.DataFrame(results, columns=[\n",
    "    'URL_ID', 'URL',                       # Columns from Input.xlsx\n",
    "    'POSITIVE SCORE', 'NEGATIVE SCORE',    # Computed metrics\n",
    "    'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
    "    'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "    'COMPLEX WORD COUNT', 'WORD COUNT',\n",
    "    'SYLLABLE PER WORD', 'PERSONAL PRONOUNS',\n",
    "    'AVG WORD LENGTH'\n",
    "])\n",
    "\n",
    "if not output_df.empty:\n",
    "    output_df.to_excel('Output Data Structure.xlsx', index=False)\n",
    "    print(\"Results saved to 'Output Data Structure.xlsx'\")\n",
    "else:\n",
    "    print(\"No valid data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ‚úÖ Conclusion <a id=\"conclusion\"></a>\n",
    "\n",
    "   This notebook automates the extraction and analysis of web articles, providing comprehensive insights through sentiment and readability metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nbformat in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (5.10.4)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (7.16.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (3.1.5)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (0.10.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (24.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (2.18.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbconvert) (1.4.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=2.6->nbformat) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=2.6->nbformat) (0.22.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (308)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from nbclient>=0.5.0->nbconvert) (8.6.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4->nbconvert) (2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\smt93\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nbformat nbconvert\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "# Read the notebook\n",
    "with open(\"Text_Extraction_and_Analysis.ipynb\") as f:\n",
    "    notebook_content = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Convert to Python script\n",
    "exporter = PythonExporter()\n",
    "python_script, _ = exporter.from_notebook_node(notebook_content)\n",
    "\n",
    "# Save to a .py file\n",
    "with open(\"Text_Extraction_and_Analysis.py\", \"w\") as f:\n",
    "    f.write(python_script)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
